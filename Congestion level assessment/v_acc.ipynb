{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75010fa5-91f8-4532-888c-e1aef19be0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = r'D:\\Anaconda\\envs\\TrKG\\Library\\share\\proj'\n",
    "os.environ['GDAL_DATA'] = r'CD:\\Anaconda\\envs\\TrKG\\Library\\share'\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d5397-3f05-4a13-85a7-09cd52dca775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Node, Relationship, Graph, NodeMatcher, RelationshipMatcher, Subgraph\n",
    "\n",
    "g = Graph('bolt://localhost:7687',auth=('neo4j','your password'), name = 'trkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9d4ba2-f0db-498a-baa0-8faa868b429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "from shapely.geometry import Point, LineString, MultiLineString, MultiPoint, GeometryCollection\n",
    "from shapely import geometry\n",
    "from shapely import ops\n",
    "from shapely.ops import split, linemerge\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505e37b6-d1b8-4a2c-a887-d171d92a1622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Derived Projected CRS: PROJCS[\"CGCS2000_3-degree_Gauss-Kruger_CM_120E\",GE ...>\n",
       "Name: CGCS2000_3-degree_Gauss-Kruger_CM_120E\n",
       "Axis Info [cartesian]:\n",
       "- [east]: Easting (metre)\n",
       "- [north]: Northing (metre)\n",
       "Area of Use:\n",
       "- undefined\n",
       "Coordinate Operation:\n",
       "- name: unnamed\n",
       "- method: Transverse Mercator\n",
       "Datum: China 2000\n",
       "- Ellipsoid: CGCS2000\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定文件路径\n",
    "crs_path = r\"D:/paper2/result/roadcrs.txt\"\n",
    "# 读取投影信息\n",
    "with open(crs_path, \"r\") as file:\n",
    "    crs_text = file.read().strip()\n",
    "\n",
    "# 将文本投影信息转换为 CRS 对象\n",
    "roadcrs = CRS.from_string(crs_text)\n",
    "roadcrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5efb019c-7662-4330-a529-9a75fb22b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从本地加载 honeycomb_cache\n",
    "with open('D:/paper2/result/honeycomb_cache.pkl', 'rb') as file:\n",
    "    honeycomb_cache = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c03c65-6fa7-4c07-9749-a9b84ba04f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid_time_summary(node_df):\n",
    "    \"\"\"\n",
    "    使用 NumPy 优化 groupby 聚合过程\n",
    "    \"\"\"\n",
    "    # 提前去重，获取每个 honeycomb_name 对应的固定值\n",
    "    honeycomb_distances = node_df['honeycomb_name'].drop_duplicates()\n",
    "\n",
    "    # 分组并用 NumPy 操作实现聚合\n",
    "    grouped = node_df.groupby('honeycomb_name')\n",
    "    start_time = grouped['datetime'].min()\n",
    "    end_time = grouped['datetime'].max()\n",
    "    min_name = grouped['trajectory_name'].min()\n",
    "    max_name = grouped['trajectory_name'].max()\n",
    "    count = max_name - min_name + 1\n",
    "\n",
    "\n",
    "    # 创建 DataFrame\n",
    "    fid_time_summary = pd.DataFrame({\n",
    "        'honeycomb_name': start_time.index,\n",
    "        'start_time': start_time.values,\n",
    "        'end_time': end_time.values,\n",
    "        'count': count.values,\n",
    "        'min_name': min_name.values,\n",
    "        'max_name': max_name.values\n",
    "    })\n",
    "\n",
    "    # 过滤 count > 1\n",
    "    fid_time_summary = fid_time_summary[fid_time_summary['count'] > 1]\n",
    "\n",
    "    # 合并 max_distance 和 min_distance\n",
    "    fid_time_summary = fid_time_summary.merge(honeycomb_distances, on='honeycomb_name', how='left')\n",
    "\n",
    "    # 计算持续时间\n",
    "    fid_time_summary['duration'] = (fid_time_summary['end_time'] - fid_time_summary['start_time']).dt.total_seconds()\n",
    "\n",
    "    # 按 start_time 排序\n",
    "    fid_time_summary = fid_time_summary.sort_values(by='min_name', ignore_index=True)\n",
    "\n",
    "    return fid_time_summary\n",
    "\n",
    "def get_honeycomb_within(honeycomb_cache, honeycomb_name):\n",
    "    \"\"\"\n",
    "    检查在本地缓存中是否存在指定的 honeycomb_name，并返回 True 如果找到。\n",
    "    \n",
    "    :param honeycomb_cache: 本地加载的 honeycomb 缓存字典\n",
    "    :param honeycomb_name: 当前 honeycomb 节点的 name\n",
    "    :return: True 如果存在 honeycomb_name，False 否则\n",
    "    \"\"\"\n",
    "    # 如果在缓存中找到指定的 honeycomb_name，立即返回 True\n",
    "    return honeycomb_name in honeycomb_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dec5e48-61b1-4de9-a93a-a71474f14c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定文件的完整路径\n",
    "file_path = r'D:\\paper2\\result\\folder_name_list.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    # 读取文件内容\n",
    "    content = file.read()\n",
    "    \n",
    "    # 将字符串转换为列表\n",
    "    folder_name = ast.literal_eval(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174e71e6-89e8-4ae4-832a-84c3f213ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_files第一个元素: 1158\n",
      "selected_files最后一个元素: 4502\n"
     ]
    }
   ],
   "source": [
    "selected_files = folder_name[1156:4500]\n",
    "print(\"selected_files第一个元素:\", selected_files[0])\n",
    "print(\"selected_files最后一个元素:\", selected_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b51d567-11bb-4db6-bea9-5bee1c2dd884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder_name第一个元素: 1158\n",
      "folder_name最后一个元素: 4502\n"
     ]
    }
   ],
   "source": [
    "selected_files = folder_name[1156:4500]\n",
    "folder_name = selected_files\n",
    "print(\"folder_name第一个元素:\", folder_name[0])\n",
    "print(\"folder_name最后一个元素:\", folder_name[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a56049-2195-4feb-97ea-57ddd0c8983a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████████████████████████████████████████| 28892160/28892160 [109:45:42<00:00, 73.12slice/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # 导入 tqdm\n",
    "import gc  # 导入垃圾回收模块\n",
    "\n",
    "# 确保日志目录存在\n",
    "log_dir = r\"C:\\DCLASS\\myJupyter\\paper2\\shanghai_log\\1\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_path = os.path.join(log_dir, \"vehicle_log_2.txt\")\n",
    "\n",
    "\n",
    "batch_size = 250\n",
    "# 计算总的进度条数：每个文件名有 8640 次迭代\n",
    "total_slices = len(folder_name) * 8640\n",
    "\n",
    "# 使用 tqdm 进度条\n",
    "with tqdm(total=total_slices, desc=\"Processing data\", unit=\"slice\") as pbar:\n",
    "    for vehicle_name in folder_name:\n",
    "        \n",
    "        # if vehicle_name != '1355':\n",
    "        #     continue\n",
    "        # if vehicle_name == '3':\n",
    "        #     break\n",
    "        # if vehicle_name != '1354':\n",
    "        #     continue\n",
    "        \n",
    "        # 🔽 写入日志\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "            log_file.write(f\"vehicle_name:\\n {vehicle_name}\\n\")\n",
    "\n",
    "        for five_times_start in range(1, 8641, batch_size):\n",
    "            five_times_end = min(five_times_start + batch_size - 1, 8640)\n",
    "            \n",
    "            # # # 记得注释\n",
    "            # if five_times_start>batch_size:\n",
    "            #     break\n",
    "\n",
    "            # 构造批次范围条件\n",
    "            batch_range = ', '.join([f\"'batch_{i}'\" for i in range(five_times_start, five_times_end + 1)])\n",
    "            \n",
    "            # 参数化查询，查找 trajectory_point 的属性以及其关联的 honeycomb 节点的部分属性\n",
    "            cypher = f\"\"\"\n",
    "                MATCH (tp:trajectory_point)-[:located_in]->(hc:honeycomb)\n",
    "                WHERE tp.vehicle = {vehicle_name} AND tp.batch IN [{batch_range}]\n",
    "                AND (tp)-[:next]-()\n",
    "                RETURN tp.batch AS batch, tp.geometry AS geometry, tp.name AS trajectory_name, tp.vehicle AS vehicle, tp.datetime AS datetime,\n",
    "                       hc.name AS honeycomb_name\n",
    "                ORDER BY tp.name\n",
    "            \"\"\"\n",
    "            # 执行查询并传入参数\n",
    "            result = g.run(cypher).data()\n",
    "\n",
    "            # 检查是否有返回数据\n",
    "            if result:\n",
    "                # 将结果转换为 DataFrame\n",
    "                df_result = pd.DataFrame(result)\n",
    "                # 确保 datetime 列是 datetime 格式\n",
    "                df_result['datetime'] = pd.to_datetime(df_result['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "                #print(f\"df_result:\\n {df_result}\")\n",
    "                \n",
    "                batch_boundaries = df_result['batch'].ne(df_result['batch'].shift()).cumsum()  # 变化检测\n",
    "                # 按变化边界分块\n",
    "\n",
    "                for _, group_idx in df_result.groupby(batch_boundaries).groups.items():\n",
    "                    node_df = df_result.loc[group_idx]\n",
    "                    batch_name = node_df['batch'].iloc[0]  # 每个分块的 batch_name 是一致的\n",
    "                \n",
    "                    # trajectory_point 节点数大于 1 才进行拥堵分析\n",
    "                    if len(node_df) > 1:\n",
    "                        fid_time_summary = calculate_fid_time_summary(node_df)\n",
    "                        # print(f\"fid_time_summary:\\n {fid_time_summary}\")\n",
    "                        # print(f\"batch_name:\\n {batch_name}\")\n",
    "                        \n",
    "                        if not fid_time_summary.empty:\n",
    "                            # 将 'geometry' 列中的 WKT 字符串转换为 Shapely 几何对象\n",
    "                            node_df['geometry'] = node_df['geometry'].apply(wkt.loads)\n",
    "                            # 转换为 GeoDataFrame\n",
    "                            node_df = gpd.GeoDataFrame(node_df, geometry='geometry', crs=roadcrs)\n",
    "                            # 设置 'trajectory_name' 为索引\n",
    "                            node_df.set_index('trajectory_name', inplace=True)\n",
    "\n",
    "                            # 遍历 fid_time_summary 的每一行\n",
    "                            for index, row in fid_time_summary.iterrows():\n",
    "                                honeycomb_name = row['honeycomb_name']\n",
    "                                # print(f\"\\nRow {index} with FID = {honeycomb_name}\")\n",
    "                                duration = row['duration']\n",
    "                    \n",
    "                                # 检查 duration 是否为 0\n",
    "                                if duration == 0:\n",
    "                                    continue  # 跳过当前行\n",
    "                    \n",
    "    \n",
    "                                # 调用 get_honeycomb_within 判断\n",
    "                                if not get_honeycomb_within(honeycomb_cache, honeycomb_name):\n",
    "                                    continue\n",
    "                                    \n",
    "                                min_name = row['min_name']\n",
    "                                max_name = row['max_name']\n",
    "                                # 直接使用 min_name 和 max_name 作为行索引进行切片\n",
    "                                between_rows = node_df.loc[min_name:max_name]\n",
    "                                # print(f\"between_rows:\\n {between_rows}\")\n",
    "            \n",
    "                                # 提取几何点并构建 LineString\n",
    "                                line = LineString(between_rows['geometry'].tolist())\n",
    "                                \n",
    "                                # 计算总长度\n",
    "                                total_length = line.length\n",
    "                                # print(f\"Total length of the point set: {total_length}\")\n",
    "                                        \n",
    "                                # 如果总长度小于 10 米，视为泊车状态，废弃该数据;如果总长度大于 600 米，视为错误数据，废弃该数据\n",
    "                                if total_length <= 20 or total_length >= 600:\n",
    "                                    continue\n",
    "                    \n",
    "                                # 计算平均速度 v_acc\n",
    "                                v_acc = total_length / duration  # 确保单位匹配\n",
    "                                \n",
    "                                # 更新匹配的节点\n",
    "                                cypher_update = f\"\"\"\n",
    "                                MATCH (tp:trajectory_point)\n",
    "                                WHERE tp.vehicle = {vehicle_name} \n",
    "                                  AND tp.batch = '{batch_name}'\n",
    "                                  AND tp.name >= {min_name} AND tp.name <= {max_name}\n",
    "                                SET tp.speed = {v_acc:.3f}\n",
    "                                \"\"\"\n",
    "                                g.run(cypher_update)\n",
    "\n",
    "                                # 输出调试信息\n",
    "                                # print(f\"Average speed (v_acc): {v_acc}\")\n",
    "\n",
    "                # 清除 node_df 缓存\n",
    "                if 'node_df' in globals():\n",
    "                    del node_df\n",
    "                if 'fid_time_summary' in globals():\n",
    "                    del fid_time_summary\n",
    "                gc.collect()  # 手动垃圾回收\n",
    "\n",
    "            # 清除 df_result 缓存\n",
    "            if 'df_result' in globals():\n",
    "                del df_result\n",
    "            if 'result' in globals():\n",
    "                del result\n",
    "            gc.collect()  # 手动垃圾回收\n",
    "\n",
    "            # 更新进度条\n",
    "            pbar.update(five_times_end - five_times_start + 1)\n",
    "\n",
    "    # 最后一次全局垃圾回收\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739c0b3-5072-41ca-ac5b-620f3efabde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44299c-a827-46e4-a3fc-dc9bc0dfc74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5bf72-6da3-47b2-94a4-ef36b330d06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
